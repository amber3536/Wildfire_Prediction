{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amber3536/Wildfire_Prediction/blob/main/Resunet2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1lEn9YU30Vah"
      },
      "outputs": [],
      "source": [
        "\"\"\"Definition of ResUNet architecture\"\"\"\n",
        "# Taken from https://github.com/nikhilroxtomar/Deep-Residual-Unet/blob/master/Deep%20Residual%20UNet.ipynb\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "def bn_act(x, act=True):\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    if act == True:\n",
        "        x = keras.layers.Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "def conv_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    conv = bn_act(x)\n",
        "    conv = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n",
        "    return conv\n",
        "\n",
        "def stem(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    conv = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
        "    conv = conv_block(conv, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
        "\n",
        "    shortcut = keras.layers.Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
        "    shortcut = bn_act(shortcut, act=False)\n",
        "\n",
        "    output = keras.layers.Add()([conv, shortcut])\n",
        "    return output\n",
        "\n",
        "def residual_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    res = conv_block(x, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
        "    res = conv_block(res, filters, kernel_size=kernel_size, padding=padding, strides=1)\n",
        "\n",
        "    shortcut = keras.layers.Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
        "    shortcut = bn_act(shortcut, act=False)\n",
        "\n",
        "    output = keras.layers.Add()([shortcut, res])\n",
        "    return output\n",
        "\n",
        "def upsample_concat_block(x, xskip):\n",
        "    u = keras.layers.UpSampling2D((2, 2))(x)\n",
        "    c = keras.layers.Concatenate()([u, xskip])\n",
        "    return c\n",
        "\n",
        "def get_model(input_shape):\n",
        "    f = [16, 32, 64, 128, 256]\n",
        "    inputs = keras.layers.Input((input_shape[0], input_shape[1], input_shape[2]))\n",
        "\n",
        "    ## Encoder\n",
        "    e0 = inputs\n",
        "    e1 = stem(e0, f[0])\n",
        "    e2 = residual_block(e1, f[1], strides=2)\n",
        "    e3 = residual_block(e2, f[2], strides=2)\n",
        "    e4 = residual_block(e3, f[3], strides=2)\n",
        "    e5 = residual_block(e4, f[4], strides=2)\n",
        "\n",
        "    ## Bridge\n",
        "    b0 = conv_block(e5, f[4], strides=1)\n",
        "    b1 = conv_block(b0, f[4], strides=1)\n",
        "\n",
        "    ## Decoder\n",
        "    u1 = upsample_concat_block(b1, e4)\n",
        "    d1 = residual_block(u1, f[4])\n",
        "\n",
        "    u2 = upsample_concat_block(d1, e3)\n",
        "    d2 = residual_block(u2, f[3])\n",
        "\n",
        "    u3 = upsample_concat_block(d2, e2)\n",
        "    d3 = residual_block(u3, f[2])\n",
        "\n",
        "    u4 = upsample_concat_block(d3, e1)\n",
        "    d4 = residual_block(u4, f[1])\n",
        "\n",
        "    outputs = keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "    model = keras.models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "# Focal Tversky_loss\n",
        "def class_tversky(y_true, y_pred):\n",
        "    smooth = 1\n",
        "\n",
        "    y_true = K.permute_dimensions(y_true, (3,1,2,0))\n",
        "    y_pred = K.permute_dimensions(y_pred, (3,1,2,0))\n",
        "\n",
        "    y_true_pos = K.batch_flatten(y_true)\n",
        "    y_pred_pos = K.batch_flatten(y_pred)\n",
        "    true_pos = K.sum(y_true_pos * y_pred_pos, 1)\n",
        "    false_neg = K.sum(y_true_pos * (1-y_pred_pos), 1)\n",
        "    false_pos = K.sum((1-y_true_pos)*y_pred_pos, 1)\n",
        "    alpha = 0.7\n",
        "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
        "\n",
        "def focal_tversky_loss(y_true,y_pred):\n",
        "    pt_1 = class_tversky(y_true, y_pred)\n",
        "    gamma = 0.75\n",
        "    return K.sum(K.pow((1-pt_1), gamma))\n",
        "\n",
        "# Dice Loss\n",
        "smooth = 1.\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1-dice_coef(y_true, y_pred)\n",
        "\n",
        "#Keras\n",
        "ALPHA = 0.8\n",
        "GAMMA = 2\n",
        "\n",
        "def focal_loss(targets, inputs, alpha=ALPHA, gamma=GAMMA):\n",
        "\n",
        "    inputs = K.flatten(inputs)\n",
        "    targets = K.flatten(targets)\n",
        "\n",
        "    BCE = K.binary_crossentropy(targets, inputs)\n",
        "    BCE_EXP = K.exp(-BCE)\n",
        "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
        "\n",
        "    return focal_loss\n",
        "\n",
        "def dice_coef_binary(y_true, y_pred, smooth=1e-7):\n",
        "    '''\n",
        "    Dice coefficient for 2 categories. Ignores background pixel label 0\n",
        "    Pass to model as metric during compile statement\n",
        "    '''\n",
        "    y_true_f = K.flatten(K.one_hot(K.cast(y_true, 'int32'), num_classes=2)[...,1:])\n",
        "    y_pred_f = K.flatten(y_pred[...,1:])\n",
        "    intersect = K.sum(y_true_f * y_pred_f, axis=-1)\n",
        "    denom = K.sum(y_true_f + y_pred_f, axis=-1)\n",
        "    return K.mean((2. * intersect / (denom + smooth)))\n",
        "\n",
        "\n",
        "def dice_coef_binary_loss(y_true, y_pred):\n",
        "    '''\n",
        "    Dice loss to minimize. Pass to model as loss during compile statement\n",
        "    '''\n",
        "    return 1 - dice_coef_binary(y_true, y_pred)\n",
        "\n",
        "def get_loss_function(loss_function_name):\n",
        "    if loss_function_name == \"focal_tversky_loss\":\n",
        "        loss_function = focal_tversky_loss\n",
        "    elif loss_function_name == \"dice_coef_loss\":\n",
        "        loss_function = dice_coef_loss\n",
        "    elif loss_function_name == \"dice_coef_binary_loss\":\n",
        "        loss_function = dice_coef_binary_loss\n",
        "    elif loss_function_name == \"focal_loss\":\n",
        "        loss_function = focal_loss\n",
        "    elif loss_function_name == \"sparse_categorical_crossentropy\":\n",
        "        loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    else:\n",
        "        loss_function = loss_function_name # for keras implemented losses like \"categorical_crossentropy\"\n",
        "\n",
        "    return loss_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CefbOwFK0wcK"
      },
      "outputs": [],
      "source": [
        "INPUT_FEATURES = ['elevation', 'wind_speed', 'wind_dir', 'tmin', 'tmax',\n",
        "                  'landcover', 'precip', 'pdsi','solar', 'PrevFireMask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "cFUBxDOj0aT7",
        "outputId": "edb8f1f7-6f05-479f-b6af-65000040aaef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.37.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231126_201833-suy79p39</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cse291-fire-prediction/fire-model/runs/suy79p39' target=\"_blank\">upbeat-meadow-15</a></strong> to <a href='https://wandb.ai/cse291-fire-prediction/fire-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cse291-fire-prediction/fire-model' target=\"_blank\">https://wandb.ai/cse291-fire-prediction/fire-model</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cse291-fire-prediction/fire-model/runs/suy79p39' target=\"_blank\">https://wandb.ai/cse291-fire-prediction/fire-model/runs/suy79p39</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from typing import Dict, List, Optional, Text, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "\n",
        "import tensorflow as tf\n",
        "# import model_satunet\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "# Get loss function\n",
        "loss_function = get_loss_function('dice_coef_loss')\n",
        "\n",
        "# Define model architecture\n",
        "model = get_model([64,64,10])\n",
        "\n",
        "# Callbacks\n",
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "callbacks = list()\n",
        "\n",
        "# Optional: WandB callback config and init\n",
        "config = {\n",
        "    \"dataset_id\": \"NDFP_data\",\n",
        "    \"img_size\": [64,64],\n",
        "    \"model_architecture\": 'resunet',\n",
        "    \"num_layers_satunet\": 4,\n",
        "    \"unfreeze_all_layers\": False,\n",
        "    \"parent_model_name\": None,\n",
        "    \"optimizer\": 'adam',\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"loss_function\": 'dice_coef_loss',\n",
        "    \"epochs\": 100,\n",
        "    \"batch_size\": 100,\n",
        "    \"custom_objects\": [\n",
        "        \"dice_coef\",\n",
        "        \"focal_tversky_loss\"\n",
        "        ],\n",
        "    \"input_features\": INPUT_FEATURES\n",
        "    }\n",
        "wandb.init(project='fire-model', config=config)\n",
        "run_name = wandb.run.name\n",
        "callbacks.append(WandbCallback())\n",
        "\n",
        "# Define learning rate schedule callback\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    0.0001, decay_steps=15, decay_rate=0.96, staircase=True\n",
        "    )\n",
        "\n",
        "#Define checkpoints callback\n",
        "checkpoint_path = os.path.join('./output', 'model', 'fire_model_{}.h5'.format(run_name))\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_path, save_weights_only=True, save_best_only=True\n",
        "    )\n",
        "callbacks.append(checkpoint_cb)\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDIcE9esrcyW",
        "outputId": "5df6e3c2-cb27-465a-8cb5-6d7f6c339220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Skvq_gnir47n"
      },
      "outputs": [],
      "source": [
        "man_length = 21550\n",
        "\n",
        "MinMax = {'landcover':(10.0, 100.0),\n",
        "              'tmax': (-41.95404185202824, 35.28747487720835),\n",
        "              'tmin': (-42.4077221254351, 34.710974191122716),\n",
        "              'wind_speed': (0.0002993076576944125, 14.27423496286687),\n",
        "              'elevation': (-77.85292, 4379.4683),\n",
        "              'wind_direction': (-179.99999083334689, 179.99999810000588),\n",
        "              'solar_radiation': (-24.598765964771623, 964662.1183104622),\n",
        "              'air_pressure': (99064.73425290409, 105551.80433948596),\n",
        "              'precipitation': (-9.494998975299606e-05, 0.012117456275976952)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MN-ZfeFdr-yv"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import gzip\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(arr, min_value, max_value):\n",
        "  arr = np.array(arr)\n",
        "  return (arr - min_value) / (max_value - min_value)"
      ],
      "metadata": {
        "id": "Cix8jFfcMna7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tQbinGk_sZ4V"
      },
      "outputs": [],
      "source": [
        "def get_data_frp(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_frp/today_frp_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_tomorrow_data_frp(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/tomorrow_frp/tomorrow_frp_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_data_fire(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_fire/today_fire_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_tomorrow_data_fire(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/tomorrow_fire/tomorrow_fire_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_data_landcover(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/landcover/today_landcover_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_data_elevation(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/elevation/today_elevation_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_wind_speed(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_wind_speed/today_wind_speed_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_air_pressure(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_air_pressure/today_air_pressure_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_wind_direction(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_wind_direction/today_wind_direction_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_temp_max(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_tmax/today_tmax_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_temp_min(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_tmin/today_tmin_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_solar_radiation(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/solar_radiation/today_solar_radiation_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_precipitation(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/precipitation/today_precipitation_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OsaBfKKrxDfB"
      },
      "outputs": [],
      "source": [
        "def _float_feature(value):\n",
        "    \"\"\"Returns a float_list from a list of float / double.\"\"\"\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def serialize_example(feature1, feature2, feature3, feature4, feature5, feature6, feature7, feature8, feature9, feature10, label):\n",
        "    \"\"\"\n",
        "    Creates a tf.train.Example message ready to be written to a file.\n",
        "    \"\"\"\n",
        "    feature = {\n",
        "        'feature1': _float_feature(np.array(feature1).flatten()),\n",
        "        'feature2': _float_feature(np.array(feature2).flatten()),\n",
        "        'feature3': _float_feature(np.array(feature3).flatten()),\n",
        "        'feature4': _float_feature(np.array(feature4).flatten()),\n",
        "        'feature5': _float_feature(np.array(feature5).flatten()),\n",
        "        'feature6': _float_feature(np.array(feature6).flatten()),\n",
        "        'feature7': _float_feature(np.array(feature7).flatten()),\n",
        "        'feature8': _float_feature(np.array(feature8).flatten()),\n",
        "        'feature9': _float_feature(np.array(feature9).flatten()),\n",
        "        'feature10': _float_feature(np.array(feature10).flatten()),\n",
        "        'label': _float_feature(np.array(label).flatten()),\n",
        "    }\n",
        "\n",
        "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return example_proto.SerializeToString()"
      ],
      "metadata": {
        "id": "CoWjN1Y-aVQF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This code cell below is for writing to Google Colab. DO NOT uncomment"
      ],
      "metadata": {
        "id": "XPbxP3raq0-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of images and labels in your dataset\n",
        "#num_examples = 21550\n",
        "#num_examples = 20\n",
        "\n",
        "# # Output file name\n",
        "# tfrecord_file = 'drive/MyDrive/file/data2.tfrecords'\n",
        "\n",
        "# with tf.io.TFRecordWriter(tfrecord_file) as writer:\n",
        "#     for idx in range(num_examples):\n",
        "#         # Load your 64x64 feature image and label\n",
        "#         #feature0, feature1, feature2, feature3,  label = load_data(i)  # Implement this function\n",
        "#         feature1 = get_data_fire(idx)\n",
        "#         feature2 = normalize(get_data_elevation(idx), MinMax[\"elevation\"][0], MinMax[\"elevation\"][1])\n",
        "#         feature3 = normalize(get_wind_speed(idx), MinMax[\"wind_speed\"][0], MinMax[\"wind_speed\"][1])\n",
        "#         feature4 = normalize(get_air_pressure(idx), MinMax[\"air_pressure\"][0], MinMax[\"air_pressure\"][1])\n",
        "#         feature5 = normalize(get_wind_direction(idx), MinMax[\"wind_direction\"][0], MinMax[\"wind_direction\"][1])\n",
        "#         feature6 = normalize(get_temp_max(idx), MinMax[\"tmax\"][0], MinMax[\"tmax\"][1])\n",
        "#         feature7 = normalize(get_temp_min(idx), MinMax[\"tmin\"][0], MinMax[\"tmin\"][1])\n",
        "#         feature8 = normalize(get_solar_radiation(idx), MinMax[\"solar_radiation\"][0], MinMax[\"solar_radiation\"][1])\n",
        "#         feature9 = normalize(get_precipitation(idx), MinMax[\"precipitation\"][0], MinMax[\"precipitation\"][1])\n",
        "#         feature10 = normalize(get_data_landcover(idx), MinMax[\"landcover\"][0], MinMax[\"landcover\"][1])\n",
        "#         label = get_tomorrow_data_fire(idx)\n",
        "#         # Serialize example and write it to the TFRecord file\n",
        "#         example = serialize_example(feature1, feature2, feature3, feature4, feature5, feature6, feature7, feature8, feature9, feature10, label)\n",
        "#         writer.write(example)\n"
      ],
      "metadata": {
        "id": "EeXlyudobsXC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _parse_function(proto):\n",
        "\n",
        "    keys_to_features = {\n",
        "        'feature1': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature2': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature3': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature4': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature5': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature6': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature7': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature8': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature9': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature10': tf.io.VarLenFeature(tf.float32),\n",
        "        'label': tf.io.VarLenFeature(tf.float32)\n",
        "    }\n",
        "\n",
        "    # Load one example\n",
        "    parsed_features = tf.io.parse_single_example(proto, keys_to_features)\n",
        "\n",
        "\n",
        "    feature_list = []\n",
        "    for i in range(1, 11):\n",
        "        feature = tf.zeros([64, 64], tf.float32)\n",
        "        tensor = tf.sparse.to_dense(parsed_features[f'feature{i}'])\n",
        "        if tf.shape(tensor) == [4096]:\n",
        "            feature = tf.reshape(tf.sparse.to_dense(parsed_features[f'feature{i}']), [64, 64])\n",
        "        feature_list.append(feature)\n",
        "\n",
        "    combined_features = tf.stack(feature_list, axis=-1)\n",
        "    label = tf.zeros([64, 64], tf.float32)\n",
        "    tensor = tf.sparse.to_dense(parsed_features[f'label'])\n",
        "    if tf.shape(tensor) == [4096]:\n",
        "        label = tf.reshape(tf.sparse.to_dense(parsed_features['label']), [64, 64])\n",
        "\n",
        "\n",
        "    return combined_features, label"
      ],
      "metadata": {
        "id": "qHGcvn5ollyY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_records(tfrecord_file):\n",
        "    count = 0\n",
        "    for _ in tf.data.TFRecordDataset(tfrecord_file):\n",
        "        count += 1\n",
        "    return count"
      ],
      "metadata": {
        "id": "p2pvDiocBKkQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_records('drive/MyDrive/file/data.tfrecords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5kRPVCFRb_2",
        "outputId": "3f042272-18a1-4151-c9ed-a45772cfd8b2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21550"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "z0gq-c1dwgiD"
      },
      "outputs": [],
      "source": [
        "tfrecord_file = 'drive/MyDrive/file/data.tfrecords'\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
        "\n",
        "dataset = dataset.map(_parse_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "cewmKGsRx8MM"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.shuffle(buffer_size=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "HbvEUFpAyZzR"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.7 * man_length)\n",
        "val_size = int(0.2 * man_length)\n",
        "\n",
        "#Split the dataset\n",
        "train_dataset = dataset.take(train_size)\n",
        "remaining_dataset = dataset.skip(train_size)\n",
        "eval_dataset = remaining_dataset.take(val_size)\n",
        "test_dataset = remaining_dataset.skip(val_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "eval_dataset = eval_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "aRWubYSkpfOC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for features, label in eval_dataset.take(1):\n",
        "    print(\"Eval batch shape:\", features.shape)\n",
        "for features, label in train_dataset.take(1):\n",
        "    print(\"Train batch shape:\", features.shape)\n",
        "for features, label in test_dataset.take(1):\n",
        "    print(\"Test batch shape:\", features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds8pm2bUkKPb",
        "outputId": "b1445454-cb51-4836-cd72-d8999764fbe4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval batch shape: (32, 64, 64, 10)\n",
            "Train batch shape: (32, 64, 64, 10)\n",
            "Test batch shape: (32, 64, 64, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO8QLFA33vPz",
        "outputId": "f45c07e4-3eb5-4328-ac83-f6de9ccee689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        }
      ],
      "source": [
        "# Compile and train model\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_function, metrics=[dice_coef,\n",
        "                                 tf.keras.metrics.AUC(curve=\"PR\"),\n",
        "                                 tf.keras.metrics.Precision(),\n",
        "                                 tf.keras.metrics.Recall()\n",
        "                                ]\n",
        "    )\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=eval_dataset,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/wrsLOx67naPQfWuNa6Zi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}