{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvhLTdM5tWANUpFstXFh0O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amber3536/Wildfire_Prediction/blob/main/Resunet2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1lEn9YU30Vah"
      },
      "outputs": [],
      "source": [
        "\"\"\"Definition of ResUNet architecture\"\"\"\n",
        "# Taken from https://github.com/nikhilroxtomar/Deep-Residual-Unet/blob/master/Deep%20Residual%20UNet.ipynb\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "def bn_act(x, act=True):\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    if act == True:\n",
        "        x = keras.layers.Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "def conv_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    conv = bn_act(x)\n",
        "    conv = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n",
        "    return conv\n",
        "\n",
        "def stem(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    conv = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
        "    conv = conv_block(conv, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
        "\n",
        "    shortcut = keras.layers.Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
        "    shortcut = bn_act(shortcut, act=False)\n",
        "\n",
        "    output = keras.layers.Add()([conv, shortcut])\n",
        "    return output\n",
        "\n",
        "def residual_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    res = conv_block(x, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
        "    res = conv_block(res, filters, kernel_size=kernel_size, padding=padding, strides=1)\n",
        "\n",
        "    shortcut = keras.layers.Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
        "    shortcut = bn_act(shortcut, act=False)\n",
        "\n",
        "    output = keras.layers.Add()([shortcut, res])\n",
        "    return output\n",
        "\n",
        "def upsample_concat_block(x, xskip):\n",
        "    u = keras.layers.UpSampling2D((2, 2))(x)\n",
        "    c = keras.layers.Concatenate()([u, xskip])\n",
        "    return c\n",
        "\n",
        "def get_model(input_shape):\n",
        "    f = [16, 32, 64, 128, 256]\n",
        "    inputs = keras.layers.Input((input_shape[0], input_shape[1], input_shape[2]))\n",
        "\n",
        "    ## Encoder\n",
        "    e0 = inputs\n",
        "    e1 = stem(e0, f[0])\n",
        "    e2 = residual_block(e1, f[1], strides=2)\n",
        "    e3 = residual_block(e2, f[2], strides=2)\n",
        "    e4 = residual_block(e3, f[3], strides=2)\n",
        "    e5 = residual_block(e4, f[4], strides=2)\n",
        "\n",
        "    ## Bridge\n",
        "    b0 = conv_block(e5, f[4], strides=1)\n",
        "    b1 = conv_block(b0, f[4], strides=1)\n",
        "\n",
        "    ## Decoder\n",
        "    u1 = upsample_concat_block(b1, e4)\n",
        "    d1 = residual_block(u1, f[4])\n",
        "\n",
        "    u2 = upsample_concat_block(d1, e3)\n",
        "    d2 = residual_block(u2, f[3])\n",
        "\n",
        "    u3 = upsample_concat_block(d2, e2)\n",
        "    d3 = residual_block(u3, f[2])\n",
        "\n",
        "    u4 = upsample_concat_block(d3, e1)\n",
        "    d4 = residual_block(u4, f[1])\n",
        "\n",
        "    outputs = keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "    model = keras.models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "# Focal Tversky_loss\n",
        "def class_tversky(y_true, y_pred):\n",
        "    smooth = 1\n",
        "\n",
        "    y_true = K.permute_dimensions(y_true, (3,1,2,0))\n",
        "    y_pred = K.permute_dimensions(y_pred, (3,1,2,0))\n",
        "\n",
        "    y_true_pos = K.batch_flatten(y_true)\n",
        "    y_pred_pos = K.batch_flatten(y_pred)\n",
        "    true_pos = K.sum(y_true_pos * y_pred_pos, 1)\n",
        "    false_neg = K.sum(y_true_pos * (1-y_pred_pos), 1)\n",
        "    false_pos = K.sum((1-y_true_pos)*y_pred_pos, 1)\n",
        "    alpha = 0.7\n",
        "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
        "\n",
        "def focal_tversky_loss(y_true,y_pred):\n",
        "    pt_1 = class_tversky(y_true, y_pred)\n",
        "    gamma = 0.75\n",
        "    return K.sum(K.pow((1-pt_1), gamma))\n",
        "\n",
        "# Dice Loss\n",
        "smooth = 1.\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1-dice_coef(y_true, y_pred)\n",
        "\n",
        "#Keras\n",
        "ALPHA = 0.8\n",
        "GAMMA = 2\n",
        "\n",
        "def focal_loss(targets, inputs, alpha=ALPHA, gamma=GAMMA):\n",
        "\n",
        "    inputs = K.flatten(inputs)\n",
        "    targets = K.flatten(targets)\n",
        "\n",
        "    BCE = K.binary_crossentropy(targets, inputs)\n",
        "    BCE_EXP = K.exp(-BCE)\n",
        "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
        "\n",
        "    return focal_loss\n",
        "\n",
        "def dice_coef_binary(y_true, y_pred, smooth=1e-7):\n",
        "    '''\n",
        "    Dice coefficient for 2 categories. Ignores background pixel label 0\n",
        "    Pass to model as metric during compile statement\n",
        "    '''\n",
        "    y_true_f = K.flatten(K.one_hot(K.cast(y_true, 'int32'), num_classes=2)[...,1:])\n",
        "    y_pred_f = K.flatten(y_pred[...,1:])\n",
        "    intersect = K.sum(y_true_f * y_pred_f, axis=-1)\n",
        "    denom = K.sum(y_true_f + y_pred_f, axis=-1)\n",
        "    return K.mean((2. * intersect / (denom + smooth)))\n",
        "\n",
        "\n",
        "def dice_coef_binary_loss(y_true, y_pred):\n",
        "    '''\n",
        "    Dice loss to minimize. Pass to model as loss during compile statement\n",
        "    '''\n",
        "    return 1 - dice_coef_binary(y_true, y_pred)\n",
        "\n",
        "def get_loss_function(loss_function_name):\n",
        "    if loss_function_name == \"focal_tversky_loss\":\n",
        "        loss_function = focal_tversky_loss\n",
        "    elif loss_function_name == \"dice_coef_loss\":\n",
        "        loss_function = dice_coef_loss\n",
        "    elif loss_function_name == \"dice_coef_binary_loss\":\n",
        "        loss_function = dice_coef_binary_loss\n",
        "    elif loss_function_name == \"focal_loss\":\n",
        "        loss_function = focal_loss\n",
        "    elif loss_function_name == \"sparse_categorical_crossentropy\":\n",
        "        loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    else:\n",
        "        loss_function = loss_function_name # for keras implemented losses like \"categorical_crossentropy\"\n",
        "\n",
        "    return loss_function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_FEATURES = ['elevation', 'wind_speed', 'wind_dir', 'tmin', 'tmax',\n",
        "                  'landcover', 'precip', 'pdsi','solar', 'PrevFireMask']"
      ],
      "metadata": {
        "id": "CefbOwFK0wcK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Dict, List, Optional, Text, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "\n",
        "import tensorflow as tf\n",
        "# import model_satunet\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "# Get loss function\n",
        "loss_function = get_loss_function('dice_coef_loss')\n",
        "\n",
        "# Define model architecture\n",
        "model = get_model([64,64,10])\n",
        "\n",
        "# Callbacks\n",
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "callbacks = list()\n",
        "\n",
        "# Optional: WandB callback config and init\n",
        "config = {\n",
        "    \"dataset_id\": \"NDFP_data\",\n",
        "    \"img_size\": [64,64],\n",
        "    \"model_architecture\": 'resunet',\n",
        "    \"num_layers_satunet\": 4,\n",
        "    \"unfreeze_all_layers\": False,\n",
        "    \"parent_model_name\": None,\n",
        "    \"optimizer\": 'adam',\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"loss_function\": 'dice_coef_loss',\n",
        "    \"epochs\": 100,\n",
        "    \"batch_size\": 100,\n",
        "    \"custom_objects\": [\n",
        "        \"dice_coef\",\n",
        "        \"focal_tversky_loss\"\n",
        "        ],\n",
        "    \"input_features\": INPUT_FEATURES\n",
        "    }\n",
        "wandb.init(project='fire-model', config=config)\n",
        "run_name = wandb.run.name\n",
        "callbacks.append(WandbCallback())\n",
        "\n",
        "# Define learning rate schedule callback\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    0.0001, decay_steps=15, decay_rate=0.96, staircase=True\n",
        "    )\n",
        "\n",
        "# Define checkpoints callback\n",
        "checkpoint_path = os.path.join('./output', 'model', 'fire_model_{}.h5'.format(run_name))\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_path, save_weights_only=True, save_best_only=True\n",
        "    )\n",
        "callbacks.append(checkpoint_cb)\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "cFUBxDOj0aT7",
        "outputId": "53df27ec-31ef-4f97-b883-6f06e928d4c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.36.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamcmains\u001b[0m (\u001b[33mcse291-fire-prediction\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231124_010602-accdsw9j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cse291-fire-prediction/fire-model/runs/accdsw9j' target=\"_blank\">easy-sunset-7</a></strong> to <a href='https://wandb.ai/cse291-fire-prediction/fire-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cse291-fire-prediction/fire-model' target=\"_blank\">https://wandb.ai/cse291-fire-prediction/fire-model</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cse291-fire-prediction/fire-model/runs/accdsw9j' target=\"_blank\">https://wandb.ai/cse291-fire-prediction/fire-model/runs/accdsw9j</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_dataset(file_pattern: Text, data_size: int, sample_size: int,\n",
        "#                 batch_size: int, num_in_channels: int, compression_type: Text,\n",
        "#                 clip_and_normalize: bool, clip_and_rescale: bool,\n",
        "#                 random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n",
        "#     \"\"\"Gets the dataset from the file pattern.\n",
        "\n",
        "#     Args:\n",
        "#     file_pattern: Input file pattern.\n",
        "#     data_size: Size of tiles (square) as read from input files.\n",
        "#     sample_size: Size the tiles (square) when input into the model.\n",
        "#     batch_size: Batch size.\n",
        "#     num_in_channels: Number of input channels.\n",
        "#     compression_type: Type of compression used for the input files.\n",
        "#     clip_and_normalize: True if the data should be clipped and normalized, False\n",
        "#       otherwise.\n",
        "#     clip_and_rescale: True if the data should be clipped and rescaled, False\n",
        "#       otherwise.\n",
        "#     random_crop: True if the data should be randomly cropped.\n",
        "#     center_crop: True if the data shoulde be cropped in the center.\n",
        "\n",
        "#     Returns:\n",
        "#     A TensorFlow dataset loaded from the input file pattern, with features\n",
        "#     described in the constants, and with the shapes determined from the input\n",
        "#     parameters to this function.\n",
        "#     \"\"\"\n",
        "#     if (clip_and_normalize and clip_and_rescale):\n",
        "#         raise ValueError('Cannot have both normalize and rescale.')\n",
        "#     dataset = tf.data.Dataset.list_files(file_pattern)\n",
        "#     dataset = dataset.interleave(\n",
        "#       lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type),\n",
        "#       num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "#     dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "#     dataset = dataset.map(\n",
        "#       lambda x: _parse_fn(  # pylint: disable=g-long-lambda\n",
        "#           x, data_size, sample_size, num_in_channels, clip_and_normalize,\n",
        "#           clip_and_rescale, random_crop, center_crop),\n",
        "#       num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "#     dataset = dataset.batch(batch_size)\n",
        "#     dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "#     return dataset"
      ],
      "metadata": {
        "id": "BpUbYaGf42N6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = get_dataset(\n",
        "#       file_pattern,\n",
        "#       data_size=64,\n",
        "#       sample_size=32,\n",
        "#       batch_size=100,\n",
        "#       num_in_channels=12,\n",
        "#       compression_type=None,\n",
        "#       clip_and_normalize=False,\n",
        "#       clip_and_rescale=False,\n",
        "#       random_crop=True,\n",
        "#       center_crop=False)\n",
        "\n",
        "# eval_dataset = get_dataset(\n",
        "#       file_pattern_eval,\n",
        "#       data_size=64,\n",
        "#       sample_size=32,\n",
        "#       batch_size=100,\n",
        "#       num_in_channels=12,\n",
        "#       compression_type=None,\n",
        "#       clip_and_normalize=False,\n",
        "#       clip_and_rescale=False,\n",
        "#       random_crop=True,\n",
        "#       center_crop=False)"
      ],
      "metadata": {
        "id": "4M6WOTsb3gaU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDIcE9esrcyW",
        "outputId": "490ec340-91f3-4a72-d2a3-cac002c7d8cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "man_length = 21550"
      ],
      "metadata": {
        "id": "Skvq_gnir47n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import gzip"
      ],
      "metadata": {
        "id": "MN-ZfeFdr-yv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_frp(i):\n",
        "  u = []\n",
        "  #with open(\"drive/MyDrive/day_fire_data/today_frp_\" + str(i) + \".csv\", mode ='r')as file:\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_frp/today_frp_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_tomorrow_data_frp(i):\n",
        "  u = []\n",
        "  #with open(\"drive/MyDrive/day_fire_data/today_frp_\" + str(i) + \".csv\", mode ='r')as file:\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/tomorrow_frp/tomorrow_frp_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_data_fire(i):\n",
        "  u = []\n",
        "  #with open(\"drive/MyDrive/day_fire_data/today_frp_\" + str(i) + \".csv\", mode ='r')as file:\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_fire/today_fire_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_tomorrow_data_fire(i):\n",
        "  u = []\n",
        "  #with open(\"drive/MyDrive/day_fire_data/today_frp_\" + str(i) + \".csv\", mode ='r')as file:\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/tomorrow_fire/tomorrow_fire_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_data_landcover(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/landcover/today_landcover_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_data_elevation(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/elevation/today_elevation_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_wind_speed(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_wind_speed/today_wind_speed_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_air_pressure(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_air_pressure/today_air_pressure_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_wind_direction(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_wind_direction/today_wind_direction_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_temp_max(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_tmax/today_tmax_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_temp_min(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_tmin/today_tmin_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_solar_radiation(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/solar_radiation/today_solar_radiation_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_precipitation(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/precipitation/today_precipitation_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u"
      ],
      "metadata": {
        "id": "tQbinGk_sZ4V"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get_data_fire(0)"
      ],
      "metadata": {
        "id": "OsaBfKKrxDfB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def data_generator():\n",
        "    for idx in range(man_length):  # Replace 'n' with the total number of file\n",
        "      feature1 = get_data_fire(idx)\n",
        "      feature2 = get_data_elevation(idx)\n",
        "      feature3 = get_wind_speed(idx)\n",
        "      feature4 = get_air_pressure(idx)\n",
        "      feature5 = get_wind_direction(idx)\n",
        "      feature6 = get_temp_max(idx)\n",
        "      feature7 = get_temp_min(idx)\n",
        "      feature8 = get_solar_radiation(idx)\n",
        "      feature9 = get_precipitation(idx)\n",
        "      feature10 = get_data_landcover(idx)\n",
        "\n",
        "      label = get_tomorrow_data_fire(idx)\n",
        "      combined_features = np.stack([feature1, feature2, feature3, feature4, feature5, feature6, feature7, feature8, feature9, feature10], axis=-1)\n",
        "      yield (combined_features, label)\n",
        "        #feature1, feature2, feature3, label = read_data_from_file(idx)  # Your file reading logic"
      ],
      "metadata": {
        "id": "2RK3EyaprUX9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=((64, 64, 10), (64, 64))\n",
        ")"
      ],
      "metadata": {
        "id": "z0gq-c1dwgiD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(buffer_size=1000).batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "cewmKGsRx8MM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.7 * man_length)\n",
        "val_size = int(0.2 * man_length)\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset = dataset.take(train_size)\n",
        "remaining_dataset = dataset.skip(train_size)\n",
        "eval_dataset = remaining_dataset.take(val_size)\n",
        "test_dataset = remaining_dataset.skip(val_size)"
      ],
      "metadata": {
        "id": "HbvEUFpAyZzR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for features, label in train_dataset.take(1):\n",
        "#     print(\"Features shape:\", features.shape)  # Should match your model's input shape\n",
        "#     print(\"Label shape:\", label.shape)"
      ],
      "metadata": {
        "id": "QmjGVuwF1W_t"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train model\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_function, metrics=[dice_coef,\n",
        "                                 tf.keras.metrics.AUC(curve=\"PR\"),\n",
        "                                 tf.keras.metrics.Precision(),\n",
        "                                 tf.keras.metrics.Recall()\n",
        "                                ]\n",
        "    )\n",
        "\n",
        "model.fit(train_dataset, epochs=5)\n",
        "# history = model.fit(\n",
        "#     train_dataset,\n",
        "#     validation_data=eval_dataset,\n",
        "#     epochs=100,\n",
        "#     callbacks=callbacks\n",
        "# )"
      ],
      "metadata": {
        "id": "hO8QLFA33vPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0bdb712-62be-4cfd-8e67-61bf88e7dbfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        }
      ]
    }
  ]
}