{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amber3536/Wildfire_Prediction/blob/main/Resunet2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1lEn9YU30Vah"
      },
      "outputs": [],
      "source": [
        "\"\"\"Definition of ResUNet architecture\"\"\"\n",
        "# Taken from https://github.com/nikhilroxtomar/Deep-Residual-Unet/blob/master/Deep%20Residual%20UNet.ipynb\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "def bn_act(x, act=True):\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    if act == True:\n",
        "        x = keras.layers.Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "def conv_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    conv = bn_act(x)\n",
        "    conv = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n",
        "    return conv\n",
        "\n",
        "def stem(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    conv = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
        "    conv = conv_block(conv, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
        "\n",
        "    shortcut = keras.layers.Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
        "    shortcut = bn_act(shortcut, act=False)\n",
        "\n",
        "    output = keras.layers.Add()([conv, shortcut])\n",
        "    return output\n",
        "\n",
        "def residual_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    res = conv_block(x, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
        "    res = conv_block(res, filters, kernel_size=kernel_size, padding=padding, strides=1)\n",
        "\n",
        "    shortcut = keras.layers.Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
        "    shortcut = bn_act(shortcut, act=False)\n",
        "\n",
        "    output = keras.layers.Add()([shortcut, res])\n",
        "    return output\n",
        "\n",
        "def upsample_concat_block(x, xskip):\n",
        "    u = keras.layers.UpSampling2D((2, 2))(x)\n",
        "    c = keras.layers.Concatenate()([u, xskip])\n",
        "    return c\n",
        "\n",
        "def get_model(input_shape):\n",
        "    f = [16, 32, 64, 128, 256]\n",
        "    inputs = keras.layers.Input((input_shape[0], input_shape[1], input_shape[2]))\n",
        "\n",
        "    ## Encoder\n",
        "    e0 = inputs\n",
        "    e1 = stem(e0, f[0])\n",
        "    e2 = residual_block(e1, f[1], strides=2)\n",
        "    e3 = residual_block(e2, f[2], strides=2)\n",
        "    e4 = residual_block(e3, f[3], strides=2)\n",
        "    e5 = residual_block(e4, f[4], strides=2)\n",
        "\n",
        "    ## Bridge\n",
        "    b0 = conv_block(e5, f[4], strides=1)\n",
        "    b1 = conv_block(b0, f[4], strides=1)\n",
        "\n",
        "    ## Decoder\n",
        "    u1 = upsample_concat_block(b1, e4)\n",
        "    d1 = residual_block(u1, f[4])\n",
        "\n",
        "    u2 = upsample_concat_block(d1, e3)\n",
        "    d2 = residual_block(u2, f[3])\n",
        "\n",
        "    u3 = upsample_concat_block(d2, e2)\n",
        "    d3 = residual_block(u3, f[2])\n",
        "\n",
        "    u4 = upsample_concat_block(d3, e1)\n",
        "    d4 = residual_block(u4, f[1])\n",
        "\n",
        "    outputs = keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "    model = keras.models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "# Focal Tversky_loss\n",
        "def class_tversky(y_true, y_pred):\n",
        "    smooth = 1\n",
        "\n",
        "    y_true = K.permute_dimensions(y_true, (3,1,2,0))\n",
        "    y_pred = K.permute_dimensions(y_pred, (3,1,2,0))\n",
        "\n",
        "    y_true_pos = K.batch_flatten(y_true)\n",
        "    y_pred_pos = K.batch_flatten(y_pred)\n",
        "    true_pos = K.sum(y_true_pos * y_pred_pos, 1)\n",
        "    false_neg = K.sum(y_true_pos * (1-y_pred_pos), 1)\n",
        "    false_pos = K.sum((1-y_true_pos)*y_pred_pos, 1)\n",
        "    alpha = 0.7\n",
        "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
        "\n",
        "def focal_tversky_loss(y_true,y_pred):\n",
        "    pt_1 = class_tversky(y_true, y_pred)\n",
        "    gamma = 0.75\n",
        "    return K.sum(K.pow((1-pt_1), gamma))\n",
        "\n",
        "# Dice Loss\n",
        "smooth = 1.\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1-dice_coef(y_true, y_pred)\n",
        "\n",
        "#Keras\n",
        "ALPHA = 0.8\n",
        "GAMMA = 2\n",
        "\n",
        "def focal_loss(targets, inputs, alpha=ALPHA, gamma=GAMMA):\n",
        "\n",
        "    inputs = K.flatten(inputs)\n",
        "    targets = K.flatten(targets)\n",
        "\n",
        "    BCE = K.binary_crossentropy(targets, inputs)\n",
        "    BCE_EXP = K.exp(-BCE)\n",
        "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
        "\n",
        "    return focal_loss\n",
        "\n",
        "def dice_coef_binary(y_true, y_pred, smooth=1e-7):\n",
        "    '''\n",
        "    Dice coefficient for 2 categories. Ignores background pixel label 0\n",
        "    Pass to model as metric during compile statement\n",
        "    '''\n",
        "    y_true_f = K.flatten(K.one_hot(K.cast(y_true, 'int32'), num_classes=2)[...,1:])\n",
        "    y_pred_f = K.flatten(y_pred[...,1:])\n",
        "    intersect = K.sum(y_true_f * y_pred_f, axis=-1)\n",
        "    denom = K.sum(y_true_f + y_pred_f, axis=-1)\n",
        "    return K.mean((2. * intersect / (denom + smooth)))\n",
        "\n",
        "\n",
        "def dice_coef_binary_loss(y_true, y_pred):\n",
        "    '''\n",
        "    Dice loss to minimize. Pass to model as loss during compile statement\n",
        "    '''\n",
        "    return 1 - dice_coef_binary(y_true, y_pred)\n",
        "\n",
        "def get_loss_function(loss_function_name):\n",
        "    if loss_function_name == \"focal_tversky_loss\":\n",
        "        loss_function = focal_tversky_loss\n",
        "    elif loss_function_name == \"dice_coef_loss\":\n",
        "        loss_function = dice_coef_loss\n",
        "    elif loss_function_name == \"dice_coef_binary_loss\":\n",
        "        loss_function = dice_coef_binary_loss\n",
        "    elif loss_function_name == \"focal_loss\":\n",
        "        loss_function = focal_loss\n",
        "    elif loss_function_name == \"sparse_categorical_crossentropy\":\n",
        "        loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    else:\n",
        "        loss_function = loss_function_name # for keras implemented losses like \"categorical_crossentropy\"\n",
        "\n",
        "    return loss_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CefbOwFK0wcK"
      },
      "outputs": [],
      "source": [
        "INPUT_FEATURES = ['elevation', 'wind_speed', 'wind_dir', 'tmin', 'tmax',\n",
        "                  'landcover', 'precip', 'pdsi','solar', 'PrevFireMask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "cFUBxDOj0aT7",
        "outputId": "0b938b50-4f66-4b5d-a538-8316c1d60b10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.37.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231125_201623-3p5lr58c</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cse291-fire-prediction/fire-model/runs/3p5lr58c' target=\"_blank\">dark-vortex-12</a></strong> to <a href='https://wandb.ai/cse291-fire-prediction/fire-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cse291-fire-prediction/fire-model' target=\"_blank\">https://wandb.ai/cse291-fire-prediction/fire-model</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cse291-fire-prediction/fire-model/runs/3p5lr58c' target=\"_blank\">https://wandb.ai/cse291-fire-prediction/fire-model/runs/3p5lr58c</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from typing import Dict, List, Optional, Text, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "\n",
        "import tensorflow as tf\n",
        "# import model_satunet\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "# Get loss function\n",
        "loss_function = get_loss_function('dice_coef_loss')\n",
        "\n",
        "# Define model architecture\n",
        "model = get_model([64,64,10])\n",
        "\n",
        "# Callbacks\n",
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "callbacks = list()\n",
        "\n",
        "# Optional: WandB callback config and init\n",
        "config = {\n",
        "    \"dataset_id\": \"NDFP_data\",\n",
        "    \"img_size\": [64,64],\n",
        "    \"model_architecture\": 'resunet',\n",
        "    \"num_layers_satunet\": 4,\n",
        "    \"unfreeze_all_layers\": False,\n",
        "    \"parent_model_name\": None,\n",
        "    \"optimizer\": 'adam',\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"loss_function\": 'dice_coef_loss',\n",
        "    \"epochs\": 100,\n",
        "    \"batch_size\": 100,\n",
        "    \"custom_objects\": [\n",
        "        \"dice_coef\",\n",
        "        \"focal_tversky_loss\"\n",
        "        ],\n",
        "    \"input_features\": INPUT_FEATURES\n",
        "    }\n",
        "wandb.init(project='fire-model', config=config)\n",
        "run_name = wandb.run.name\n",
        "callbacks.append(WandbCallback())\n",
        "\n",
        "# Define learning rate schedule callback\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    0.0001, decay_steps=15, decay_rate=0.96, staircase=True\n",
        "    )\n",
        "\n",
        "#Define checkpoints callback\n",
        "checkpoint_path = os.path.join('./output', 'model', 'fire_model_{}.h5'.format(run_name))\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_path, save_weights_only=True, save_best_only=True\n",
        "    )\n",
        "callbacks.append(checkpoint_cb)\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BpUbYaGf42N6"
      },
      "outputs": [],
      "source": [
        "# def get_dataset(file_pattern: Text, data_size: int, sample_size: int,\n",
        "#                 batch_size: int, num_in_channels: int, compression_type: Text,\n",
        "#                 clip_and_normalize: bool, clip_and_rescale: bool,\n",
        "#                 random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n",
        "#     \"\"\"Gets the dataset from the file pattern.\n",
        "\n",
        "#     Args:\n",
        "#     file_pattern: Input file pattern.\n",
        "#     data_size: Size of tiles (square) as read from input files.\n",
        "#     sample_size: Size the tiles (square) when input into the model.\n",
        "#     batch_size: Batch size.\n",
        "#     num_in_channels: Number of input channels.\n",
        "#     compression_type: Type of compression used for the input files.\n",
        "#     clip_and_normalize: True if the data should be clipped and normalized, False\n",
        "#       otherwise.\n",
        "#     clip_and_rescale: True if the data should be clipped and rescaled, False\n",
        "#       otherwise.\n",
        "#     random_crop: True if the data should be randomly cropped.\n",
        "#     center_crop: True if the data shoulde be cropped in the center.\n",
        "\n",
        "#     Returns:\n",
        "#     A TensorFlow dataset loaded from the input file pattern, with features\n",
        "#     described in the constants, and with the shapes determined from the input\n",
        "#     parameters to this function.\n",
        "#     \"\"\"\n",
        "#     if (clip_and_normalize and clip_and_rescale):\n",
        "#         raise ValueError('Cannot have both normalize and rescale.')\n",
        "#     dataset = tf.data.Dataset.list_files(file_pattern)\n",
        "#     dataset = dataset.interleave(\n",
        "#       lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type),\n",
        "#       num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "#     dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "#     dataset = dataset.map(\n",
        "#       lambda x: _parse_fn(  # pylint: disable=g-long-lambda\n",
        "#           x, data_size, sample_size, num_in_channels, clip_and_normalize,\n",
        "#           clip_and_rescale, random_crop, center_crop),\n",
        "#       num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "#     dataset = dataset.batch(batch_size)\n",
        "#     dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "#     return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4M6WOTsb3gaU"
      },
      "outputs": [],
      "source": [
        "# train_dataset = get_dataset(\n",
        "#       file_pattern,\n",
        "#       data_size=64,\n",
        "#       sample_size=32,\n",
        "#       batch_size=100,\n",
        "#       num_in_channels=12,\n",
        "#       compression_type=None,\n",
        "#       clip_and_normalize=False,\n",
        "#       clip_and_rescale=False,\n",
        "#       random_crop=True,\n",
        "#       center_crop=False)\n",
        "\n",
        "# eval_dataset = get_dataset(\n",
        "#       file_pattern_eval,\n",
        "#       data_size=64,\n",
        "#       sample_size=32,\n",
        "#       batch_size=100,\n",
        "#       num_in_channels=12,\n",
        "#       compression_type=None,\n",
        "#       clip_and_normalize=False,\n",
        "#       clip_and_rescale=False,\n",
        "#       random_crop=True,\n",
        "#       center_crop=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDIcE9esrcyW",
        "outputId": "fecb209c-436b-4928-ac51-cc80bbe41b52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Skvq_gnir47n"
      },
      "outputs": [],
      "source": [
        "man_length = 21550\n",
        "\n",
        "MinMax = {'landcover':(10.0, 100.0),\n",
        "              'tmax': (-41.95404185202824, 35.28747487720835),\n",
        "              'tmin': (-42.4077221254351, 34.710974191122716),\n",
        "              'wind_speed': (0.0002993076576944125, 14.27423496286687),\n",
        "              'elevation': (-77.85292, 4379.4683),\n",
        "              'wind_direction': (-179.99999083334689, 179.99999810000588),\n",
        "              'solar_radiation': (-24.598765964771623, 964662.1183104622),\n",
        "              'air_pressure': (99064.73425290409, 105551.80433948596),\n",
        "              'precipitation': (-9.494998975299606e-05, 0.012117456275976952)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MN-ZfeFdr-yv"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import gzip\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(arr, min_value, max_value):\n",
        "  arr = np.array(arr)\n",
        "  return (arr - min_value) / (max_value - min_value)"
      ],
      "metadata": {
        "id": "Cix8jFfcMna7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tQbinGk_sZ4V"
      },
      "outputs": [],
      "source": [
        "def get_data_frp(i):\n",
        "  u = []\n",
        "  #with open(\"drive/MyDrive/day_fire_data/today_frp_\" + str(i) + \".csv\", mode ='r')as file:\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_frp/today_frp_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_tomorrow_data_frp(i):\n",
        "  u = []\n",
        "  #with open(\"drive/MyDrive/day_fire_data/today_frp_\" + str(i) + \".csv\", mode ='r')as file:\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/tomorrow_frp/tomorrow_frp_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_data_fire(i):\n",
        "  u = []\n",
        "  #with open(\"drive/MyDrive/day_fire_data/today_frp_\" + str(i) + \".csv\", mode ='r')as file:\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_fire/today_fire_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_tomorrow_data_fire(i):\n",
        "  u = []\n",
        "  #with open(\"drive/MyDrive/day_fire_data/today_frp_\" + str(i) + \".csv\", mode ='r')as file:\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/tomorrow_fire/tomorrow_fire_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_data_landcover(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/landcover/today_landcover_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_data_elevation(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/elevation/today_elevation_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_wind_speed(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_wind_speed/today_wind_speed_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_air_pressure(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_air_pressure/today_air_pressure_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_wind_direction(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_wind_direction/today_wind_direction_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_temp_max(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_tmax/today_tmax_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_temp_min(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_tmin/today_tmin_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_solar_radiation(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/solar_radiation/today_solar_radiation_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_precipitation(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/precipitation/today_precipitation_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OsaBfKKrxDfB"
      },
      "outputs": [],
      "source": [
        "def _float_feature(value):\n",
        "    \"\"\"Returns a float_list from a list of float / double.\"\"\"\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def serialize_example(feature1, feature2, feature3, feature4, feature5, feature6, feature7, feature8, feature9, feature10, label):\n",
        "    \"\"\"\n",
        "    Creates a tf.train.Example message ready to be written to a file.\n",
        "    \"\"\"\n",
        "    feature = {\n",
        "        'feature1': _float_feature(np.array(feature1).flatten()),\n",
        "        'feature2': _float_feature(np.array(feature2).flatten()),\n",
        "        'feature3': _float_feature(np.array(feature3).flatten()),\n",
        "        'feature4': _float_feature(np.array(feature4).flatten()),\n",
        "        'feature5': _float_feature(np.array(feature5).flatten()),\n",
        "        'feature6': _float_feature(np.array(feature6).flatten()),\n",
        "        'feature7': _float_feature(np.array(feature7).flatten()),\n",
        "        'feature8': _float_feature(np.array(feature8).flatten()),\n",
        "        'feature9': _float_feature(np.array(feature9).flatten()),\n",
        "        'feature10': _float_feature(np.array(feature10).flatten()),\n",
        "        'label': _float_feature(np.array(label).flatten()),\n",
        "    }\n",
        "\n",
        "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return example_proto.SerializeToString()"
      ],
      "metadata": {
        "id": "CoWjN1Y-aVQF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of images and labels in your dataset\n",
        "num_examples = 21550\n",
        "#num_examples = 20\n",
        "\n",
        "# Output file name\n",
        "tfrecord_file = 'drive/MyDrive/file/data.tfrecords'\n",
        "\n",
        "# with tf.io.TFRecordWriter(tfrecord_file) as writer:\n",
        "#     for idx in range(num_examples):\n",
        "#         # Load your 64x64 feature image and label\n",
        "#         #feature0, feature1, feature2, feature3,  label = load_data(i)  # Implement this function\n",
        "#         feature1 = get_data_fire(idx)\n",
        "#         feature2 = normalize(get_data_elevation(idx), MinMax[\"elevation\"][0], MinMax[\"elevation\"][1])\n",
        "#         feature3 = normalize(get_wind_speed(idx), MinMax[\"wind_speed\"][0], MinMax[\"wind_speed\"][1])\n",
        "#         feature4 = normalize(get_air_pressure(idx), MinMax[\"air_pressure\"][0], MinMax[\"air_pressure\"][1])\n",
        "#         feature5 = normalize(get_wind_direction(idx), MinMax[\"wind_direction\"][0], MinMax[\"wind_direction\"][1])\n",
        "#         feature6 = normalize(get_temp_max(idx), MinMax[\"tmax\"][0], MinMax[\"tmax\"][1])\n",
        "#         feature7 = normalize(get_temp_min(idx), MinMax[\"tmin\"][0], MinMax[\"tmin\"][1])\n",
        "#         feature8 = normalize(get_solar_radiation(idx), MinMax[\"solar_radiation\"][0], MinMax[\"solar_radiation\"][1])\n",
        "#         feature9 = normalize(get_precipitation(idx), MinMax[\"precipitation\"][0], MinMax[\"precipitation\"][1])\n",
        "#         feature10 = normalize(get_data_landcover(idx), MinMax[\"landcover\"][0], MinMax[\"landcover\"][1])\n",
        "#         label = get_tomorrow_data_fire(idx)\n",
        "#         # Serialize example and write it to the TFRecord file\n",
        "#         example = serialize_example(feature1, feature2, feature3, feature4, feature5, feature6, feature7, feature8, feature9, feature10, label)\n",
        "#         writer.write(example)\n"
      ],
      "metadata": {
        "id": "EeXlyudobsXC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _parse_function(proto):\n",
        "\n",
        "    # keys_to_features = {\n",
        "    #     'feature1': tf.io.VarLenFeature(tf.float32),\n",
        "    #     'feature2': tf.io.VarLenFeature(tf.float32),\n",
        "    #     'feature3': tf.io.VarLenFeature(tf.float32),\n",
        "    #     'feature4': tf.io.VarLenFeature(tf.float32),\n",
        "    #     'feature5': tf.io.VarLenFeature(tf.float32),\n",
        "    #     'feature6': tf.io.VarLenFeature(tf.float32),\n",
        "    #     'feature7': tf.io.VarLenFeature(tf.float32),\n",
        "    #     'feature8': tf.io.VarLenFeature(tf.float32),\n",
        "    #     'feature9': tf.io.VarLenFeature(tf.float32),\n",
        "    #     'feature10': tf.io.VarLenFeature(tf.float32),\n",
        "    #     'label': tf.io.VarLenFeature(tf.float32)\n",
        "    # }\n",
        "    keys_to_features = {\n",
        "        'feature1': tf.io.FixedLenFeature([64*64], tf.float32),\n",
        "        'feature2': tf.io.FixedLenFeature([64*64], tf.float32),\n",
        "        'feature3': tf.io.FixedLenFeature([64*64], tf.float32),\n",
        "        'feature4': tf.io.FixedLenFeature([64*64], tf.float32),\n",
        "        'feature5': tf.io.FixedLenFeature([64*64], tf.float32),\n",
        "        'feature6': tf.io.FixedLenFeature([64*64], tf.float32),\n",
        "        'feature7': tf.io.FixedLenFeature([64*64], tf.float32),\n",
        "        'feature8': tf.io.FixedLenFeature([64*64], tf.float32),\n",
        "        'feature9': tf.io.FixedLenFeature([64*64], tf.float32),\n",
        "        'feature10': tf.io.FixedLenFeature([64*64], tf.float32),\n",
        "        'label': tf.io.FixedLenFeature([64*64], tf.float32),\n",
        "    }\n",
        "\n",
        "    # Load one example\n",
        "    parsed_features = tf.io.parse_single_example(proto, keys_to_features)\n",
        "\n",
        "    features = [tf.reshape(parsed_features[f'feature{i}'], [64, 64]) for i in range(10)]\n",
        "    label = tf.reshape(parsed_features['label'], [64, 64])\n",
        "\n",
        "    # Stack features along a new axis if your model expects a single input tensor\n",
        "    stacked_features = tf.stack(features, axis=-1)\n",
        "    # feature_list = []\n",
        "    # for i in range(1, 11):\n",
        "    #     feature = tf.reshape(tf.sparse.to_dense(parsed_features[f'feature{i}']), [64, 64])\n",
        "    #     feature_list.append(feature)\n",
        "\n",
        "    # combined_features = tf.stack(feature_list, axis=-1)\n",
        "    # label = tf.reshape(tf.sparse.to_dense(parsed_features['label']), [64, 64])\n",
        "\n",
        "\n",
        "    return stacked_features, label\n"
      ],
      "metadata": {
        "id": "qHGcvn5ollyY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2RK3EyaprUX9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# def data_generator():\n",
        "#     for idx in range(man_length):  # Replace 'n' with the total number of file\n",
        "#       feature1 = get_data_fire(idx)\n",
        "#       feature2 = normalize(get_data_elevation(idx), MinMax[\"elevation\"][0], MinMax[\"elevation\"][1])\n",
        "#       feature3 = normalize(get_wind_speed(idx), MinMax[\"wind_speed\"][0], MinMax[\"wind_speed\"][1])\n",
        "#       feature4 = normalize(get_air_pressure(idx), MinMax[\"air_pressure\"][0], MinMax[\"air_pressure\"][1])\n",
        "#       feature5 = normalize(get_wind_direction(idx), MinMax[\"wind_direction\"][0], MinMax[\"wind_direction\"][1])\n",
        "#       feature6 = normalize(get_temp_max(idx), MinMax[\"tmax\"][0], MinMax[\"tmax\"][1])\n",
        "#       feature7 = normalize(get_temp_min(idx), MinMax[\"tmin\"][0], MinMax[\"tmin\"][1])\n",
        "#       feature8 = normalize(get_solar_radiation(idx), MinMax[\"solar_radiation\"][0], MinMax[\"solar_radiation\"][1])\n",
        "#       feature9 = normalize(get_precipitation(idx), MinMax[\"precipitation\"][0], MinMax[\"precipitation\"][1])\n",
        "#       feature10 = normalize(get_data_landcover(idx), MinMax[\"landcover\"][0], MinMax[\"landcover\"][1])\n",
        "\n",
        "#       label = get_tomorrow_data_fire(idx)\n",
        "#       combined_features = np.stack([feature1, feature2, feature3, feature4, feature5, feature6, feature7, feature8, feature9, feature10], axis=-1)\n",
        "#       yield (combined_features, label)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_records(tfrecord_file):\n",
        "    count = 0\n",
        "    for _ in tf.data.TFRecordDataset(tfrecord_file):\n",
        "        count += 1\n",
        "    return count"
      ],
      "metadata": {
        "id": "p2pvDiocBKkQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count_records(tfrecord_file)"
      ],
      "metadata": {
        "id": "MwKNMGKwBM4S"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "z0gq-c1dwgiD"
      },
      "outputs": [],
      "source": [
        "# dataset = tf.data.Dataset.from_generator(\n",
        "#     data_generator,\n",
        "#     output_types=(tf.float32, tf.float32),\n",
        "#     output_shapes=((64, 64, 10), (64, 64))\n",
        "# )\n",
        "\n",
        "tfrecord_file = 'drive/MyDrive/file/data.tfrecords'\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
        "\n",
        "dataset = dataset.map(_parse_function)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "cewmKGsRx8MM"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.shuffle(buffer_size=1000).batch(32).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HbvEUFpAyZzR"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.7 * man_length)\n",
        "val_size = int(0.2 * man_length)\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset = dataset.take(train_size)\n",
        "remaining_dataset = dataset.skip(train_size)\n",
        "eval_dataset = remaining_dataset.take(val_size)\n",
        "test_dataset = remaining_dataset.skip(val_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QmjGVuwF1W_t"
      },
      "outputs": [],
      "source": [
        "# for features, label in train_dataset.take(1):\n",
        "#     print(\"Features shape:\", features.shape)  # Should match your model's input shape\n",
        "#     print(\"Label shape:\", label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hO8QLFA33vPz",
        "outputId": "50bcaa5e-f6fc-41ea-e0d7-821d01eeadf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "    240/Unknown - 1209s 5s/step - loss: 0.8297 - dice_coef: 0.1703 - auc: 0.0579 - precision: 0.0699 - recall: 0.4237"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-80494da17d93>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#model.fit(train_dataset, epochs=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node Reshape_1 defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 250, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 748, in __init__\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-29-80494da17d93>\", line 12, in <cell line: 12>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1131, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1225, in compute_metrics\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 723, in update_state\n\n  File \"<ipython-input-8-6d2cd4daeb88>\", line 101, in dice_coef\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 3930, in flatten\n\nInput to reshape is a tensor with 1168265 values, but the requested shape has 4096\n\t [[{{node Reshape_1}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_19006]"
          ]
        }
      ],
      "source": [
        "# Compile and train model\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_function, metrics=[dice_coef,\n",
        "                                 tf.keras.metrics.AUC(curve=\"PR\"),\n",
        "                                 tf.keras.metrics.Precision(),\n",
        "                                 tf.keras.metrics.Recall()\n",
        "                                ]\n",
        "    )\n",
        "\n",
        "#model.fit(train_dataset, epochs=5)\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=eval_dataset,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRLpSpHVN6hUWSABA+Ho0s",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}