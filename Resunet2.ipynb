{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amber3536/Wildfire_Prediction/blob/main/Resunet2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1lEn9YU30Vah"
      },
      "outputs": [],
      "source": [
        "\"\"\"Definition of ResUNet architecture\"\"\"\n",
        "# Taken from https://github.com/nikhilroxtomar/Deep-Residual-Unet/blob/master/Deep%20Residual%20UNet.ipynb\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "def bn_act(x, act=True):\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    if act == True:\n",
        "        x = keras.layers.Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "def conv_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    conv = bn_act(x)\n",
        "    conv = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n",
        "    return conv\n",
        "\n",
        "def stem(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    conv = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
        "    conv = conv_block(conv, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
        "\n",
        "    shortcut = keras.layers.Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
        "    shortcut = bn_act(shortcut, act=False)\n",
        "\n",
        "    output = keras.layers.Add()([conv, shortcut])\n",
        "    return output\n",
        "\n",
        "def residual_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    res = conv_block(x, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
        "    res = conv_block(res, filters, kernel_size=kernel_size, padding=padding, strides=1)\n",
        "\n",
        "    shortcut = keras.layers.Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
        "    shortcut = bn_act(shortcut, act=False)\n",
        "\n",
        "    output = keras.layers.Add()([shortcut, res])\n",
        "    return output\n",
        "\n",
        "def upsample_concat_block(x, xskip):\n",
        "    u = keras.layers.UpSampling2D((2, 2))(x)\n",
        "    c = keras.layers.Concatenate()([u, xskip])\n",
        "    return c\n",
        "\n",
        "def get_model(input_shape):\n",
        "    f = [16, 32, 64, 128, 256]\n",
        "    inputs = keras.layers.Input((input_shape[0], input_shape[1], input_shape[2]))\n",
        "\n",
        "    ## Encoder\n",
        "    e0 = inputs\n",
        "    e1 = stem(e0, f[0])\n",
        "    e2 = residual_block(e1, f[1], strides=2)\n",
        "    e3 = residual_block(e2, f[2], strides=2)\n",
        "    e4 = residual_block(e3, f[3], strides=2)\n",
        "    e5 = residual_block(e4, f[4], strides=2)\n",
        "\n",
        "    ## Bridge\n",
        "    b0 = conv_block(e5, f[4], strides=1)\n",
        "    b1 = conv_block(b0, f[4], strides=1)\n",
        "\n",
        "    ## Decoder\n",
        "    u1 = upsample_concat_block(b1, e4)\n",
        "    d1 = residual_block(u1, f[4])\n",
        "\n",
        "    u2 = upsample_concat_block(d1, e3)\n",
        "    d2 = residual_block(u2, f[3])\n",
        "\n",
        "    u3 = upsample_concat_block(d2, e2)\n",
        "    d3 = residual_block(u3, f[2])\n",
        "\n",
        "    u4 = upsample_concat_block(d3, e1)\n",
        "    d4 = residual_block(u4, f[1])\n",
        "\n",
        "    outputs = keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "    model = keras.models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "# Focal Tversky_loss\n",
        "def class_tversky(y_true, y_pred):\n",
        "    smooth = 1\n",
        "\n",
        "    y_true = K.permute_dimensions(y_true, (3,1,2,0))\n",
        "    y_pred = K.permute_dimensions(y_pred, (3,1,2,0))\n",
        "\n",
        "    y_true_pos = K.batch_flatten(y_true)\n",
        "    y_pred_pos = K.batch_flatten(y_pred)\n",
        "    true_pos = K.sum(y_true_pos * y_pred_pos, 1)\n",
        "    false_neg = K.sum(y_true_pos * (1-y_pred_pos), 1)\n",
        "    false_pos = K.sum((1-y_true_pos)*y_pred_pos, 1)\n",
        "    alpha = 0.7\n",
        "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
        "\n",
        "def focal_tversky_loss(y_true,y_pred):\n",
        "    pt_1 = class_tversky(y_true, y_pred)\n",
        "    gamma = 0.75\n",
        "    return K.sum(K.pow((1-pt_1), gamma))\n",
        "\n",
        "# Dice Loss\n",
        "smooth = 1.\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1-dice_coef(y_true, y_pred)\n",
        "\n",
        "#Keras\n",
        "ALPHA = 0.8\n",
        "GAMMA = 2\n",
        "\n",
        "def focal_loss(targets, inputs, alpha=ALPHA, gamma=GAMMA):\n",
        "\n",
        "    inputs = K.flatten(inputs)\n",
        "    targets = K.flatten(targets)\n",
        "\n",
        "    BCE = K.binary_crossentropy(targets, inputs)\n",
        "    BCE_EXP = K.exp(-BCE)\n",
        "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
        "\n",
        "    return focal_loss\n",
        "\n",
        "def dice_coef_binary(y_true, y_pred, smooth=1e-7):\n",
        "    '''\n",
        "    Dice coefficient for 2 categories. Ignores background pixel label 0\n",
        "    Pass to model as metric during compile statement\n",
        "    '''\n",
        "    y_true_f = K.flatten(K.one_hot(K.cast(y_true, 'int32'), num_classes=2)[...,1:])\n",
        "    y_pred_f = K.flatten(y_pred[...,1:])\n",
        "    intersect = K.sum(y_true_f * y_pred_f, axis=-1)\n",
        "    denom = K.sum(y_true_f + y_pred_f, axis=-1)\n",
        "    return K.mean((2. * intersect / (denom + smooth)))\n",
        "\n",
        "\n",
        "def dice_coef_binary_loss(y_true, y_pred):\n",
        "    '''\n",
        "    Dice loss to minimize. Pass to model as loss during compile statement\n",
        "    '''\n",
        "    return 1 - dice_coef_binary(y_true, y_pred)\n",
        "\n",
        "def get_loss_function(loss_function_name):\n",
        "    if loss_function_name == \"focal_tversky_loss\":\n",
        "        loss_function = focal_tversky_loss\n",
        "    elif loss_function_name == \"dice_coef_loss\":\n",
        "        loss_function = dice_coef_loss\n",
        "    elif loss_function_name == \"dice_coef_binary_loss\":\n",
        "        loss_function = dice_coef_binary_loss\n",
        "    elif loss_function_name == \"focal_loss\":\n",
        "        loss_function = focal_loss\n",
        "    elif loss_function_name == \"sparse_categorical_crossentropy\":\n",
        "        loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    else:\n",
        "        loss_function = loss_function_name # for keras implemented losses like \"categorical_crossentropy\"\n",
        "\n",
        "    return loss_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CefbOwFK0wcK"
      },
      "outputs": [],
      "source": [
        "INPUT_FEATURES = ['elevation', 'wind_speed', 'wind_dir', 'tmin', 'tmax',\n",
        "                  'landcover', 'precip', 'pdsi','solar', 'PrevFireMask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "cFUBxDOj0aT7",
        "outputId": "1bd51343-9d84-44f1-f042-84fb6589ac38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.37.1-py2.py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.37.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from typing import Dict, List, Optional, Text, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "\n",
        "import tensorflow as tf\n",
        "# import model_satunet\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "# Get loss function\n",
        "loss_function = get_loss_function('dice_coef_loss')\n",
        "\n",
        "# Define model architecture\n",
        "model = get_model([64,64,10])\n",
        "\n",
        "# Callbacks\n",
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "callbacks = list()\n",
        "\n",
        "# Optional: WandB callback config and init\n",
        "config = {\n",
        "    \"dataset_id\": \"NDFP_data\",\n",
        "    \"img_size\": [64,64],\n",
        "    \"model_architecture\": 'resunet',\n",
        "    \"num_layers_satunet\": 4,\n",
        "    \"unfreeze_all_layers\": False,\n",
        "    \"parent_model_name\": None,\n",
        "    \"optimizer\": 'adam',\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"loss_function\": 'dice_coef_loss',\n",
        "    \"epochs\": 100,\n",
        "    \"batch_size\": 100,\n",
        "    \"custom_objects\": [\n",
        "        \"dice_coef\",\n",
        "        \"focal_tversky_loss\"\n",
        "        ],\n",
        "    \"input_features\": INPUT_FEATURES\n",
        "    }\n",
        "wandb.init(project='fire-model', config=config)\n",
        "run_name = wandb.run.name\n",
        "callbacks.append(WandbCallback())\n",
        "\n",
        "# Define learning rate schedule callback\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    0.0001, decay_steps=15, decay_rate=0.96, staircase=True\n",
        "    )\n",
        "\n",
        "#Define checkpoints callback\n",
        "checkpoint_path = os.path.join('./output', 'model', 'fire_model_{}.h5'.format(run_name))\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_path, save_weights_only=True, save_best_only=True\n",
        "    )\n",
        "callbacks.append(checkpoint_cb)\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpUbYaGf42N6"
      },
      "outputs": [],
      "source": [
        "# def get_dataset(file_pattern: Text, data_size: int, sample_size: int,\n",
        "#                 batch_size: int, num_in_channels: int, compression_type: Text,\n",
        "#                 clip_and_normalize: bool, clip_and_rescale: bool,\n",
        "#                 random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n",
        "#     \"\"\"Gets the dataset from the file pattern.\n",
        "\n",
        "#     Args:\n",
        "#     file_pattern: Input file pattern.\n",
        "#     data_size: Size of tiles (square) as read from input files.\n",
        "#     sample_size: Size the tiles (square) when input into the model.\n",
        "#     batch_size: Batch size.\n",
        "#     num_in_channels: Number of input channels.\n",
        "#     compression_type: Type of compression used for the input files.\n",
        "#     clip_and_normalize: True if the data should be clipped and normalized, False\n",
        "#       otherwise.\n",
        "#     clip_and_rescale: True if the data should be clipped and rescaled, False\n",
        "#       otherwise.\n",
        "#     random_crop: True if the data should be randomly cropped.\n",
        "#     center_crop: True if the data shoulde be cropped in the center.\n",
        "\n",
        "#     Returns:\n",
        "#     A TensorFlow dataset loaded from the input file pattern, with features\n",
        "#     described in the constants, and with the shapes determined from the input\n",
        "#     parameters to this function.\n",
        "#     \"\"\"\n",
        "#     if (clip_and_normalize and clip_and_rescale):\n",
        "#         raise ValueError('Cannot have both normalize and rescale.')\n",
        "#     dataset = tf.data.Dataset.list_files(file_pattern)\n",
        "#     dataset = dataset.interleave(\n",
        "#       lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type),\n",
        "#       num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "#     dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "#     dataset = dataset.map(\n",
        "#       lambda x: _parse_fn(  # pylint: disable=g-long-lambda\n",
        "#           x, data_size, sample_size, num_in_channels, clip_and_normalize,\n",
        "#           clip_and_rescale, random_crop, center_crop),\n",
        "#       num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "#     dataset = dataset.batch(batch_size)\n",
        "#     dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "#     return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M6WOTsb3gaU"
      },
      "outputs": [],
      "source": [
        "# train_dataset = get_dataset(\n",
        "#       file_pattern,\n",
        "#       data_size=64,\n",
        "#       sample_size=32,\n",
        "#       batch_size=100,\n",
        "#       num_in_channels=12,\n",
        "#       compression_type=None,\n",
        "#       clip_and_normalize=False,\n",
        "#       clip_and_rescale=False,\n",
        "#       random_crop=True,\n",
        "#       center_crop=False)\n",
        "\n",
        "# eval_dataset = get_dataset(\n",
        "#       file_pattern_eval,\n",
        "#       data_size=64,\n",
        "#       sample_size=32,\n",
        "#       batch_size=100,\n",
        "#       num_in_channels=12,\n",
        "#       compression_type=None,\n",
        "#       clip_and_normalize=False,\n",
        "#       clip_and_rescale=False,\n",
        "#       random_crop=True,\n",
        "#       center_crop=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDIcE9esrcyW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skvq_gnir47n"
      },
      "outputs": [],
      "source": [
        "man_length = 21550\n",
        "\n",
        "MinMax = {'landcover':(10.0, 100.0),\n",
        "              'tmax': (-41.95404185202824, 35.28747487720835),\n",
        "              'tmin': (-42.4077221254351, 34.710974191122716),\n",
        "              'wind_speed': (0.0002993076576944125, 14.27423496286687),\n",
        "              'elevation': (-77.85292, 4379.4683),\n",
        "              'wind_direction': (-179.99999083334689, 179.99999810000588),\n",
        "              'solar_radiation': (-24.598765964771623, 964662.1183104622),\n",
        "              'air_pressure': (99064.73425290409, 105551.80433948596),\n",
        "              'precipitation': (-9.494998975299606e-05, 0.012117456275976952)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN-ZfeFdr-yv"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import gzip\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(arr, min_value, max_value):\n",
        "  arr = np.array(arr)\n",
        "  return (arr - min_value) / (max_value - min_value)"
      ],
      "metadata": {
        "id": "Cix8jFfcMna7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQbinGk_sZ4V"
      },
      "outputs": [],
      "source": [
        "def get_data_frp(i):\n",
        "  u = []\n",
        "  #with open(\"drive/MyDrive/day_fire_data/today_frp_\" + str(i) + \".csv\", mode ='r')as file:\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_frp/today_frp_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_tomorrow_data_frp(i):\n",
        "  u = []\n",
        "  #with open(\"drive/MyDrive/day_fire_data/today_frp_\" + str(i) + \".csv\", mode ='r')as file:\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/tomorrow_frp/tomorrow_frp_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_data_fire(i):\n",
        "  u = []\n",
        "  #with open(\"drive/MyDrive/day_fire_data/today_frp_\" + str(i) + \".csv\", mode ='r')as file:\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_fire/today_fire_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_tomorrow_data_fire(i):\n",
        "  u = []\n",
        "  #with open(\"drive/MyDrive/day_fire_data/today_frp_\" + str(i) + \".csv\", mode ='r')as file:\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/tomorrow_fire/tomorrow_fire_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_data_landcover(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/landcover/today_landcover_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_data_elevation(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/elevation/today_elevation_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_wind_speed(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_wind_speed/today_wind_speed_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_air_pressure(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_air_pressure/today_air_pressure_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_wind_direction(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_wind_direction/today_wind_direction_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_temp_max(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_tmax/today_tmax_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_temp_min(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/today_tmin/today_tmin_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_solar_radiation(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/solar_radiation/today_solar_radiation_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u\n",
        "\n",
        "def get_precipitation(i):\n",
        "  u = []\n",
        "  with gzip.open(\"drive/MyDrive/file/day_fire_data/precipitation/today_precipitation_\" + str(i) + \".gz\", mode ='rt')as file:\n",
        "    csvFile = csv.reader(file)\n",
        "    for lines in csvFile:\n",
        "      p = []\n",
        "      for y in lines:\n",
        "        p.append(float(y))\n",
        "      u.append(p)\n",
        "  return u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsaBfKKrxDfB"
      },
      "outputs": [],
      "source": [
        "def _float_feature(value):\n",
        "    \"\"\"Returns a float_list from a list of float / double.\"\"\"\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def serialize_example(feature1, feature2, feature3, feature4, feature5, feature6, feature7, feature8, feature9, feature10, label):\n",
        "    \"\"\"\n",
        "    Creates a tf.train.Example message ready to be written to a file.\n",
        "    \"\"\"\n",
        "    feature = {\n",
        "        'feature1': _float_feature(np.array(feature1).flatten()),\n",
        "        'feature2': _float_feature(np.array(feature2).flatten()),\n",
        "        'feature3': _float_feature(np.array(feature3).flatten()),\n",
        "        'feature4': _float_feature(np.array(feature4).flatten()),\n",
        "        'feature5': _float_feature(np.array(feature5).flatten()),\n",
        "        'feature6': _float_feature(np.array(feature6).flatten()),\n",
        "        'feature7': _float_feature(np.array(feature7).flatten()),\n",
        "        'feature8': _float_feature(np.array(feature8).flatten()),\n",
        "        'feature9': _float_feature(np.array(feature9).flatten()),\n",
        "        'feature10': _float_feature(np.array(feature10).flatten()),\n",
        "        'label': _float_feature(np.array(label).flatten()),\n",
        "    }\n",
        "\n",
        "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return example_proto.SerializeToString()"
      ],
      "metadata": {
        "id": "CoWjN1Y-aVQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of images and labels in your dataset\n",
        "num_examples = 21550\n",
        "#num_examples = 20\n",
        "\n",
        "# Output file name\n",
        "tfrecord_file = 'drive/MyDrive/file/data.tfrecords'\n",
        "\n",
        "# with tf.io.TFRecordWriter(tfrecord_file) as writer:\n",
        "#     for idx in range(num_examples):\n",
        "#         # Load your 64x64 feature image and label\n",
        "#         #feature0, feature1, feature2, feature3,  label = load_data(i)  # Implement this function\n",
        "#         feature1 = get_data_fire(idx)\n",
        "#         feature2 = normalize(get_data_elevation(idx), MinMax[\"elevation\"][0], MinMax[\"elevation\"][1])\n",
        "#         feature3 = normalize(get_wind_speed(idx), MinMax[\"wind_speed\"][0], MinMax[\"wind_speed\"][1])\n",
        "#         feature4 = normalize(get_air_pressure(idx), MinMax[\"air_pressure\"][0], MinMax[\"air_pressure\"][1])\n",
        "#         feature5 = normalize(get_wind_direction(idx), MinMax[\"wind_direction\"][0], MinMax[\"wind_direction\"][1])\n",
        "#         feature6 = normalize(get_temp_max(idx), MinMax[\"tmax\"][0], MinMax[\"tmax\"][1])\n",
        "#         feature7 = normalize(get_temp_min(idx), MinMax[\"tmin\"][0], MinMax[\"tmin\"][1])\n",
        "#         feature8 = normalize(get_solar_radiation(idx), MinMax[\"solar_radiation\"][0], MinMax[\"solar_radiation\"][1])\n",
        "#         feature9 = normalize(get_precipitation(idx), MinMax[\"precipitation\"][0], MinMax[\"precipitation\"][1])\n",
        "#         feature10 = normalize(get_data_landcover(idx), MinMax[\"landcover\"][0], MinMax[\"landcover\"][1])\n",
        "#         label = get_tomorrow_data_fire(idx)\n",
        "#         # Serialize example and write it to the TFRecord file\n",
        "#         example = serialize_example(feature1, feature2, feature3, feature4, feature5, feature6, feature7, feature8, feature9, feature10, label)\n",
        "#         writer.write(example)\n"
      ],
      "metadata": {
        "id": "EeXlyudobsXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _parse_function(proto):\n",
        "\n",
        "    keys_to_features = {\n",
        "        'feature1': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature2': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature3': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature4': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature5': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature6': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature7': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature8': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature9': tf.io.VarLenFeature(tf.float32),\n",
        "        'feature10': tf.io.VarLenFeature(tf.float32),\n",
        "        'label': tf.io.VarLenFeature(tf.float32)\n",
        "    }\n",
        "\n",
        "    # Load one example\n",
        "    parsed_features = tf.io.parse_single_example(proto, keys_to_features)\n",
        "\n",
        "    feature_list = []\n",
        "    for i in range(1, 11):\n",
        "        feature = tf.reshape(tf.sparse.to_dense(parsed_features[f'feature{i}']), [64, 64])\n",
        "        feature_list.append(feature)\n",
        "\n",
        "    combined_features = tf.stack(feature_list, axis=-1)\n",
        "    label = tf.reshape(tf.sparse.to_dense(parsed_features['label']), [64, 64])\n",
        "\n",
        "\n",
        "    return combined_features, label\n"
      ],
      "metadata": {
        "id": "qHGcvn5ollyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RK3EyaprUX9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# def data_generator():\n",
        "#     for idx in range(man_length):  # Replace 'n' with the total number of file\n",
        "#       feature1 = get_data_fire(idx)\n",
        "#       feature2 = normalize(get_data_elevation(idx), MinMax[\"elevation\"][0], MinMax[\"elevation\"][1])\n",
        "#       feature3 = normalize(get_wind_speed(idx), MinMax[\"wind_speed\"][0], MinMax[\"wind_speed\"][1])\n",
        "#       feature4 = normalize(get_air_pressure(idx), MinMax[\"air_pressure\"][0], MinMax[\"air_pressure\"][1])\n",
        "#       feature5 = normalize(get_wind_direction(idx), MinMax[\"wind_direction\"][0], MinMax[\"wind_direction\"][1])\n",
        "#       feature6 = normalize(get_temp_max(idx), MinMax[\"tmax\"][0], MinMax[\"tmax\"][1])\n",
        "#       feature7 = normalize(get_temp_min(idx), MinMax[\"tmin\"][0], MinMax[\"tmin\"][1])\n",
        "#       feature8 = normalize(get_solar_radiation(idx), MinMax[\"solar_radiation\"][0], MinMax[\"solar_radiation\"][1])\n",
        "#       feature9 = normalize(get_precipitation(idx), MinMax[\"precipitation\"][0], MinMax[\"precipitation\"][1])\n",
        "#       feature10 = normalize(get_data_landcover(idx), MinMax[\"landcover\"][0], MinMax[\"landcover\"][1])\n",
        "\n",
        "#       label = get_tomorrow_data_fire(idx)\n",
        "#       combined_features = np.stack([feature1, feature2, feature3, feature4, feature5, feature6, feature7, feature8, feature9, feature10], axis=-1)\n",
        "#       yield (combined_features, label)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_records(tfrecord_file):\n",
        "    count = 0\n",
        "    for _ in tf.data.TFRecordDataset(tfrecord_file):\n",
        "        count += 1\n",
        "    return count"
      ],
      "metadata": {
        "id": "p2pvDiocBKkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count_records(tfrecord_file)"
      ],
      "metadata": {
        "id": "MwKNMGKwBM4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0gq-c1dwgiD"
      },
      "outputs": [],
      "source": [
        "# dataset = tf.data.Dataset.from_generator(\n",
        "#     data_generator,\n",
        "#     output_types=(tf.float32, tf.float32),\n",
        "#     output_shapes=((64, 64, 10), (64, 64))\n",
        "# )\n",
        "\n",
        "tfrecord_file = 'drive/MyDrive/file/data.tfrecords'\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
        "\n",
        "dataset = dataset.map(_parse_function)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cewmKGsRx8MM"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.shuffle(buffer_size=1000).batch(32).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbvEUFpAyZzR"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.7 * man_length)\n",
        "val_size = int(0.2 * man_length)\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset = dataset.take(train_size)\n",
        "remaining_dataset = dataset.skip(train_size)\n",
        "eval_dataset = remaining_dataset.take(val_size)\n",
        "test_dataset = remaining_dataset.skip(val_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmjGVuwF1W_t"
      },
      "outputs": [],
      "source": [
        "# for features, label in train_dataset.take(1):\n",
        "#     print(\"Features shape:\", features.shape)  # Should match your model's input shape\n",
        "#     print(\"Label shape:\", label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO8QLFA33vPz"
      },
      "outputs": [],
      "source": [
        "# Compile and train model\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_function, metrics=[dice_coef,\n",
        "                                 tf.keras.metrics.AUC(curve=\"PR\"),\n",
        "                                 tf.keras.metrics.Precision(),\n",
        "                                 tf.keras.metrics.Recall()\n",
        "                                ]\n",
        "    )\n",
        "\n",
        "#model.fit(train_dataset, epochs=5)\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=eval_dataset,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiDPnV+/7/q/M3VhZ/3Ocx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}